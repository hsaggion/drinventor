/*
 * ******************************************************************************************************
 * Dr. Inventor Text Mining Framework Java Library
 * 
 * This code has been developed by the Natural Language Processing Group of the
 * Universitat Pompeu Fabra in the context of the FP7 European Project Dr. Inventor
 * Call: FP7-ICT-2013.8.1 - Agreement No: 611383
 * 
 * Dr. Inventor Text Mining Framework Java Library is available under an open licence, GPLv3, for non-commercial applications.
 * ******************************************************************************************************
 */
package edu.upf.taln.dri.module.importer.pdf;


import java.util.ArrayList;
import java.util.HashSet;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.Optional;
import java.util.Set;

import org.apache.commons.lang3.StringUtils;
import org.apache.log4j.Logger;
import org.springframework.util.CollectionUtils;

import edu.upf.taln.dri.common.util.GateUtil;
import edu.upf.taln.dri.module.importer.ImporterBase;
import edu.upf.taln.dri.module.importer.SourceENUM;
import gate.Annotation;
import gate.AnnotationSet;
import gate.Document;
import gate.Factory;
import gate.FeatureMap;
import gate.creole.ExecutionException;
import gate.creole.metadata.CreoleParameter;
import gate.creole.metadata.CreoleResource;
import gate.creole.metadata.RunTime;
import gate.util.InvalidOffsetException;
import gate.util.SimpleFeatureMapImpl;

/**
 * From the XML mark-up generated by PDFX, this processing resource identifies all the textual contents,
 * split their sentences by exploiting a customized REGEXP Sentence Splitter of ANNIE
 * 
 *
 */
@CreoleResource(name = "DRI Modules - PDFX importer")
public class ImporterPDFX extends ImporterBase {

	private static Logger logger = Logger.getLogger(ImporterPDFX.class);	

	private static final long serialVersionUID = 1L;

	public static final String PDFXAnnSet = "Original markups";
	public static final String PDFXbibEntry = "ref";
	public static final String PDFXbibEntry_IdFeat = "rid";
	public static final String PDFXcitMarker = "xref";
	public static final String PDFXcitMarker_refTypeFeat = "ref-type";
	public static final String PDFXcitMarker_IdFeat = "rid";
	
	// Input and output annotation
	private String inputSentenceASname;
	private String inputSentenceAStype;

	public String getInputSentenceASname() {
		return inputSentenceASname;
	}

	@RunTime
	@CreoleParameter(defaultValue = "Analysis", comment = "The name of the input annotation set to read sentence annotations from (sentence annotations previously added by sentence splitter execution)")
	public void setInputSentenceASname(String inputSentenceASname) {
		this.inputSentenceASname = inputSentenceASname;
	}

	public String getInputSentenceAStype() {
		return inputSentenceAStype;
	}

	@RunTime
	@CreoleParameter(defaultValue = "Sentence", comment = "The name of the input annotation type to read sentence annotations (sentence annotations previously added by sentence splitter execution)")
	public void setInputSentenceAStype(String inputSentenceAStype) {
		this.inputSentenceAStype = inputSentenceAStype;
	}

	/**
	 * Internal utility function to add a sentence annotation of type outputAStypeAppo to the annotation sets outputAs
	 * and outputAsOriginal, starting at startNode, ending at endNode and with features map equal to fm.
	 * 
	 * This method, before adding a new sentence annotation, performs the following steps:
	 *    - removes the header of section eventually included in the sentence
	 *    - left and right trims the sentence span
	 *    - check for duplicated sentence annotations
	 * 
	 * @param doc
	 * @param pdfx
	 * @param outputAs
	 * @param outputAsOriginal
	 * @param outputAStypeAppo
	 * @param startNode
	 * @param endNode
	 * @param fm
	 * @return
	 */
	private Integer addSentence(Document doc, AnnotationSet pdfx, AnnotationSet outputAs, String outputAStypeAppo, Long startNode, Long endNode, FeatureMap fm) {
		Integer newSentId = null;

		try {
			// Check if not header in sentence - if header in sentence, remove it
			List<String> headersAnnName = new ArrayList<String>();
			headersAnnName.add(ImporterBase.h1AnnType);
			headersAnnName.add(ImporterBase.h2AnnType);
			headersAnnName.add(ImporterBase.h3AnnType);
			headersAnnName.add(ImporterBase.h4AnnType);
			headersAnnName.add(ImporterBase.h5AnnType);

			for(String headName : headersAnnName) {
				AnnotationSet headersAnns = outputAs.get(headName, startNode, endNode);
				if(headersAnns != null && headersAnns.size() > 0) {
					Iterator<Annotation> headersAnnsIter = headersAnns.iterator();
					while(headersAnnsIter.hasNext()) {
						Annotation headersAnnsElem = headersAnnsIter.next();
						if(headersAnnsElem != null) {
							Long endOffsetHeadersAnnsElem = headersAnnsElem.getEndNode().getOffset();
							if(endOffsetHeadersAnnsElem != null && endOffsetHeadersAnnsElem > startNode && endOffsetHeadersAnnsElem < endNode) {
								startNode = endOffsetHeadersAnnsElem;
							}

							Long startOffsetHeadersAnnsElem = headersAnnsElem.getStartNode().getOffset();
							if(startOffsetHeadersAnnsElem != null && startOffsetHeadersAnnsElem > startNode && startOffsetHeadersAnnsElem < endNode) {
								endNode = startOffsetHeadersAnnsElem;
							}
						}
					}
				}
			}

			// Remove region with class feature set to unknown
			FeatureMap constraintFeatures_regionUnknown = new SimpleFeatureMapImpl();
			constraintFeatures_regionUnknown.put("class", "unknown");
			AnnotationSet regionUnknownAnns = pdfx.get("region", constraintFeatures_regionUnknown);
			if(regionUnknownAnns != null && regionUnknownAnns.size() > 0) {
				Iterator<Annotation> regionUnknownAnnsIter = regionUnknownAnns.iterator();
				while(regionUnknownAnnsIter.hasNext()) {
					Annotation regionUnknownAnnsElem = regionUnknownAnnsIter.next();
					if(regionUnknownAnnsElem != null) {
						Long endOffsetRegionUnknownAnnsElem = regionUnknownAnnsElem.getEndNode().getOffset();
						if(endOffsetRegionUnknownAnnsElem != null && endOffsetRegionUnknownAnnsElem > startNode && endOffsetRegionUnknownAnnsElem < endNode) {
							startNode = endOffsetRegionUnknownAnnsElem;
						}

						Long startOffsetRegionUnknownAnnsElem = regionUnknownAnnsElem.getStartNode().getOffset();
						if(startOffsetRegionUnknownAnnsElem != null && startOffsetRegionUnknownAnnsElem > startNode && startOffsetRegionUnknownAnnsElem < endNode) {
							endNode = startOffsetRegionUnknownAnnsElem;
						}
					}
				}
			}

			// Remove outsider text
			AnnotationSet outsiderAnns = pdfx.get("outsider");
			if(outsiderAnns != null && outsiderAnns.size() > 0) {
				Iterator<Annotation> outsiderAnnsIter = outsiderAnns.iterator();
				while(outsiderAnnsIter.hasNext()) {
					Annotation outsiderAnnsElem = outsiderAnnsIter.next();
					if(outsiderAnnsElem != null) {
						Long startOffsetOutsiderAnnsElem = outsiderAnnsElem.getStartNode().getOffset();
						if(startOffsetOutsiderAnnsElem != null && startOffsetOutsiderAnnsElem > startNode && startOffsetOutsiderAnnsElem < endNode) {
							endNode = startOffsetOutsiderAnnsElem;
						}

						Long endOffsetOutsiderAnnsElem = outsiderAnnsElem.getEndNode().getOffset();
						if(endOffsetOutsiderAnnsElem != null && endOffsetOutsiderAnnsElem > startNode && endOffsetOutsiderAnnsElem < endNode) {
							startNode = endOffsetOutsiderAnnsElem;
						}
					}
				}
			}

			// Trim the new sentence annotation
			try {
				String sentContent = doc.getContent().getContent(startNode, endNode).toString();
				if(sentContent != null && sentContent.length() > 0) {
					for(int i = 0; i < sentContent.length(); i++) {
						char ch = sentContent.charAt(i);
						if(ch == ' ' || ch == '\n' || ch == '\t') {
							if(startNode < (endNode - 1)) {
								startNode = startNode + 1;
							}
						}
						else {
							break;
						}
					}

					for(int i = (sentContent.length() - 1); i >= 0; i--) {
						char ch = sentContent.charAt(i);
						if(ch == ' ' || ch == '\n' || ch == '\t') {
							if(startNode < (endNode - 1)) {
								endNode = endNode - 1;
							}
						}
						else {
							break;
						}
					}
				}
			}
			catch (Exception e) {
				e.printStackTrace();
			}

			// Check if the sentence is at least 10 chars long
			boolean tooFewChars = false;
			if( (endNode - startNode) < 10l) {
				tooFewChars = true;
			}

			// Check if the same sentence annotation was not already present, in order not to duplicate it
			boolean alreadyCreatedSentence = false;
			AnnotationSet asCheck = outputAs.getCovering(outputAStypeAppo, startNode, endNode);
			if(asCheck != null && asCheck.size() > 0) {
				alreadyCreatedSentence = true;
			}

			if(!tooFewChars && !alreadyCreatedSentence) {	
				newSentId = outputAs.add(startNode, endNode, outputAStypeAppo, fm);
			}

		} catch (InvalidOffsetException e) {
			logger.error("ERROR, InvalidOffsetException - " + e.getLocalizedMessage());
			e.printStackTrace();
		}

		return newSentId;
	}


	public void execute() throws ExecutionException {

		// Get the document to process
		gate.Document doc = getDocument();

		// Normalize variables
		String inputSentenceASnameAppo = (this.inputSentenceASname != null) ? this.inputSentenceASname : "";
		String inputSentenceAStypeAppo = (StringUtils.isNotBlank(this.inputSentenceAStype)) ? this.inputSentenceAStype : "Sentence";

		// Set PDFX as input type
		doc.setFeatures((doc.getFeatures() != null) ? doc.getFeatures() : Factory.newFeatureMap());
		doc.getFeatures().put("source", SourceENUM.PDFX.toString());

		// Import from PDFX annotations
		GateUtil.transferAnnotations(this.document, "abstract", ImporterBase.abstractAnnType, PDFXAnnSet, ImporterBase.driAnnSet, null);
		GateUtil.transferAnnotations(this.document, PDFXbibEntry, ImporterBase.bibEntryAnnType, PDFXAnnSet, ImporterBase.driAnnSet, null);
		GateUtil.transferAnnotations(this.document, "h1", ImporterBase.h1AnnType, PDFXAnnSet, ImporterBase.driAnnSet, null);
		GateUtil.transferAnnotations(this.document, "h2", ImporterBase.h2AnnType, PDFXAnnSet, ImporterBase.driAnnSet, null);
		GateUtil.transferAnnotations(this.document, "h3", ImporterBase.h3AnnType, PDFXAnnSet, ImporterBase.driAnnSet, null);
		GateUtil.transferAnnotations(this.document, "h4", ImporterBase.h4AnnType, PDFXAnnSet, ImporterBase.driAnnSet, null);
		GateUtil.transferAnnotations(this.document, "h5", ImporterBase.h5AnnType, PDFXAnnSet, ImporterBase.driAnnSet, null);
		GateUtil.transferAnnotations(this.document, "article-title", ImporterBase.titleAnnType, PDFXAnnSet, ImporterBase.driAnnSet, null);
		GateUtil.transferAnnotations(this.document, "caption", ImporterBase.captionAnnType, PDFXAnnSet, ImporterBase.driAnnSet, null);
		
		// Inline cits: GateUtil.transferAnnotations(this.document, "xref", ImporterBase.inlineCitationMarkerAnnType, PDFXAnnSet, ImporterBase.driAnnSet, getInlineRefs);

		// Create reference and new annotation sets
		AnnotationSet sentenceMarkup = ((inputSentenceASnameAppo != null && !inputSentenceASnameAppo.equals(""))? doc.getAnnotations(inputSentenceASnameAppo) : doc.getAnnotations());
		AnnotationSet pdfxMarkup = doc.getAnnotations(PDFXAnnSet);
		AnnotationSet outputMarkup = doc.getAnnotations(ImporterBase.driAnnSet);

		// Copy all the (inputSentenceAStypeAppo) sentences as annotations of type (inputSentenceAStypeAppo + "_OLD") and delete the original annotations
		Set<Integer> annIdOfOldSentences = new HashSet<Integer>();
		Set<Integer> annIdToDelete = new HashSet<Integer>();
		List<Annotation> inputSentenceAnnotations = gate.Utils.inDocumentOrder(sentenceMarkup.get(inputSentenceAStypeAppo));
		inputSentenceAnnotations.stream().forEach((ann) -> {
			if(ann != null) {
				ann.getFeatures().put("OLD_SENTENCE", "TO_DELETE");

				// Generate a copy of the sentence with type sufficed by "_OLD"
				try {
					Integer annToDeleteId = sentenceMarkup.add(ann.getStartNode().getOffset(), ann.getEndNode().getOffset(), inputSentenceAStypeAppo + "_OLD", ann.getFeatures());
					annIdOfOldSentences.add(annToDeleteId);
				} catch (InvalidOffsetException e) {
					e.printStackTrace();
				}

				// Delete the original annotation
				annIdToDelete.add(ann.getId());
			}
		});

		if(annIdToDelete != null && annIdToDelete.size() > 0) {
			for(Integer annIdToDel : annIdToDelete) {
				if(annIdToDel != null) {
					Annotation sentenceAnn = sentenceMarkup.get(inputSentenceAStypeAppo).get(annIdToDel);
					if(sentenceAnn != null) {
						sentenceMarkup.remove(sentenceAnn);
					}
				}
			}
		}


		// Adding sentences from the PDFX abstract annotations / XML elements
		List<Annotation> abstractAnnList = GateUtil.getAnnInDocOrder(this.document, PDFXAnnSet, "abstract");
		if(abstractAnnList.size() > 0) {
			for(Annotation abstractAnn : abstractAnnList) {

				// Go through tokens overlapping annotation and add as sentences
				AnnotationSet intersectingSentences = sentenceMarkup.get(inputSentenceAStypeAppo + "_OLD", abstractAnn.getStartNode().getOffset(), abstractAnn.getEndNode().getOffset());
				Iterator<Annotation> intersectingSentencesIter = intersectingSentences.iterator();
				while(intersectingSentencesIter.hasNext()) {
					Annotation sentenceAnn = intersectingSentencesIter.next();

					FeatureMap fm = new SimpleFeatureMapImpl();
					fm.put("PDFX_from", "abstract");

					// Import PDFX features in the new sentence annotation (names prefixed by 'PDFX_')
					for(Map.Entry<Object, Object> entry : abstractAnn.getFeatures().entrySet()) {
						try {
							String featName = (String) entry.getKey();
							fm.put("PDFX__" + featName, entry.getValue());
						}
						catch (Exception e) {

						}
					}

					Integer newSentenceId = addSentence(doc, pdfxMarkup, outputMarkup, ImporterBase.sentenceAnnType, sentenceAnn.getStartNode().getOffset(), sentenceAnn.getEndNode().getOffset(), fm);
					fm.put("gateID", newSentenceId);
				}
			}
		}


		// Adding sentences from the PDFX region annotations / XML elements with feature class=DoCO:TextChunk
		FeatureMap constraintFeatures = new SimpleFeatureMapImpl();
		constraintFeatures.put("class", "DoCO:TextChunk");
		AnnotationSet regionTextChunkSentences = pdfxMarkup.get("region", constraintFeatures);

		if(regionTextChunkSentences != null && regionTextChunkSentences.size() > 0) {
			Iterator<Annotation> regionTextChunkSentencesIter = regionTextChunkSentences.iterator();

			while(regionTextChunkSentencesIter.hasNext()) {
				Annotation regionTextChunkSentencesAnn = regionTextChunkSentencesIter.next();

				// Go through tokens overlapping annotation and add as sentences
				AnnotationSet intersectingSentences = sentenceMarkup.get(inputSentenceAStypeAppo + "_OLD", regionTextChunkSentencesAnn.getStartNode().getOffset(), regionTextChunkSentencesAnn.getEndNode().getOffset());
				Iterator<Annotation> intersectingSentencesIter = intersectingSentences.iterator();
				while(intersectingSentencesIter.hasNext()) {
					Annotation sentenceAnn = intersectingSentencesIter.next();

					FeatureMap fm = new SimpleFeatureMapImpl();
					fm.put("PDFX_from", "region_DoCO_TextChunk");

					// Import PDFX features in the new sentence annotation (names prefixed by 'PDFX_')
					// boolean confidenceTextChunkPossible = false;
					for(Map.Entry<Object, Object> entry : regionTextChunkSentencesAnn.getFeatures().entrySet()) {
						try {
							String featName = (String) entry.getKey();
							fm.put("PDFX__" + featName, entry.getValue());

							if(featName.equals("confidence")) {
								String valueFeat = (String) entry.getValue();
								if(valueFeat.equals("possible")) {
									// confidenceTextChunkPossible = true;
								}
							}
						}
						catch (Exception e) {

						}
					}

					// Exclude TextChunks with confidence = possible
					// if (!confidenceTextChunkPossible) {
					Integer newSentenceId = addSentence(doc, pdfxMarkup, outputMarkup, ImporterBase.sentenceAnnType, sentenceAnn.getStartNode().getOffset(), sentenceAnn.getEndNode().getOffset(), fm);
					fm.put("gateID", newSentenceId);
					// }

				}
			}
		}
		
		// If there is no abstract, all sentences from the first to the beginning of the first section are abstract
		List<Annotation> abstractAnnotationList = GateUtil.getAnnInDocOrder(this.document, ImporterBase.driAnnSet, ImporterBase.abstractAnnType);
		if(CollectionUtils.isEmpty(abstractAnnotationList)) {
			Optional<Annotation> h1AnnList = GateUtil.getFirstAnnotationInDocOrder(this.document, ImporterBase.driAnnSet, ImporterBase.h1AnnType);
			if(h1AnnList.isPresent()) {
				List<Annotation> abstractSentenceList = GateUtil.getAnnInDocOrderContainedOffset(this.document, ImporterBase.driAnnSet, ImporterBase.sentenceAnnType,
						0l, h1AnnList.get().getStartNode().getOffset());
				if(abstractSentenceList.size() > 0) {
					Long initialAbstractOffset = abstractSentenceList.get(0).getStartNode().getOffset();
					Long finalAbstractOffset = abstractSentenceList.get(abstractSentenceList.size() - 1).getEndNode().getOffset();
					try {
						this.document.getAnnotations(ImporterBase.driAnnSet).add(initialAbstractOffset, finalAbstractOffset, ImporterBase.abstractAnnType, Factory.newFeatureMap());
					} catch (InvalidOffsetException e) {
						/* Do nothing */
					}
				}
			}
		}
		
		// Delete all sentences and header annotations that are after the first bibliographic entry or the section headers that have more than 14 tokens
		Annotation firstBibEntryAnn = GateUtil.getFirstAnnotationInDocOrder(this.document, ImporterBase.driAnnSet, ImporterBase.bibEntryAnnType).orElse(null);
		if(firstBibEntryAnn != null) {
			Long firstBibEntryStartOffset = firstBibEntryAnn.getStartNode().getOffset();
			if(firstBibEntryStartOffset != null) {
				Set<Integer> annIdToRemove = new HashSet<Integer>();
				
				List<String> annTypesToRemove = new ArrayList<String>();
				annTypesToRemove.add(ImporterBase.sentenceAnnType);
				annTypesToRemove.add(ImporterBase.h1AnnType);
				annTypesToRemove.add(ImporterBase.h2AnnType);
				annTypesToRemove.add(ImporterBase.h3AnnType);
				annTypesToRemove.add(ImporterBase.h4AnnType);
				annTypesToRemove.add(ImporterBase.h5AnnType);
				
				for(String annTypeToRem : annTypesToRemove) {
					List<Annotation> sentenceAnnList = GateUtil.getAnnInDocOrder(this.document, ImporterBase.driAnnSet, annTypeToRem);
					for(Annotation sent : sentenceAnnList) {
						if(sent != null && sent.getStartNode().getOffset() >= firstBibEntryStartOffset && sent.getId() != null) {
							annIdToRemove.add(sent.getId());
						}
						
						// Remove incorrect headers
						if(sent.getType().equals(ImporterBase.h1AnnType) || sent.getType().equals(ImporterBase.h2AnnType) || 
							sent.getType().equals(ImporterBase.h3AnnType) || sent.getType().equals(ImporterBase.h4AnnType) || sent.getType().equals(ImporterBase.h5AnnType)) {
							
							List<Annotation> intersectingTokenAnnList = GateUtil.getAnnotationInDocumentOrderContainedAnnotation(doc, ImporterBase.driAnnSet, ImporterBase.tokenAnnType, sent);
							if(intersectingTokenAnnList.size() > 14) {
								annIdToRemove.add(sent.getId());
							}
							
							String sectionHeaderText = GateUtil.getAnnotationText(sent, doc).orElse(null);
							if(sectionHeaderText != null && (sectionHeaderText.trim().startsWith(",") || sectionHeaderText.trim().startsWith(".") ||
									sectionHeaderText.trim().startsWith(":") || sectionHeaderText.trim().startsWith(";"))) {
								annIdToRemove.add(sent.getId());
							}
						}
					}
				}
				
				if(annIdToRemove.size() > 0) {
					for(Integer annIdToRem : annIdToRemove) {
						Annotation annToRem = this.document.getAnnotations(ImporterBase.driAnnSet).get(annIdToRem);
						if(annToRem != null) {
							this.document.getAnnotations(ImporterBase.driAnnSet).remove(annToRem);
						}
					}
				}
				
			}
		}
		
		// Delete all the section headers of type h1 before the first h1 section that contains the word Introduction and is in the first 30% of the document
		List<Annotation> rootSectionList = GateUtil.getAnnInDocOrder(doc, ImporterBase.driAnnSet, ImporterBase.h1AnnType);
		if(rootSectionList != null && rootSectionList.size() > 0) {
			long deleteAllSectBefore = Long.MIN_VALUE;
			
			Annotation introRootSectionAnn = null;
			for(Annotation rootSect : rootSectionList) {
				String rootSectText = GateUtil.getAnnotationText(rootSect, doc).orElse(null);
				if(rootSect != null && rootSectText != null && rootSectText.toLowerCase().trim().contains("introduction")) {
					introRootSectionAnn = rootSect;
					break;
				}
			}
			
			// Check if the first h1 section that contains the word Introduction is in the first 30% of the document
			if(introRootSectionAnn != null) {
				long introRootSectionAnnStartOffset = introRootSectionAnn.getStartNode().getOffset();
				long docLength = gate.Utils.lengthLong(doc);
				
				double positionOfIntroRooSectionAnn = 1d;
				if(docLength > 0 && docLength > introRootSectionAnnStartOffset) {
					positionOfIntroRooSectionAnn = new Double(introRootSectionAnnStartOffset) / new Double(docLength);
				}
				if(positionOfIntroRooSectionAnn <= 0.3d) {
					deleteAllSectBefore = introRootSectionAnnStartOffset;
				}
			}
			
			if(deleteAllSectBefore > 0d) {
				List<Annotation> sectAnnToDel = new ArrayList<Annotation>();
				for(Annotation rootSect : rootSectionList) {
					if(rootSect != null && rootSect.getStartNode().getOffset() < deleteAllSectBefore) {
						sectAnnToDel.add(rootSect);
					}
				}
				
				for(Annotation sectAnnToD : sectAnnToDel) {
					if(sectAnnToD != null) {
						doc.getAnnotations(ImporterBase.driAnnSet).remove(sectAnnToD);
					}
				}
			}
		}
		
		// Remove all the annotation ids of sentences to remove - with type inputSentenceAStypeAppo + "_OLD"
		if(annIdOfOldSentences != null && annIdOfOldSentences.size() > 0) {
			for(Integer annIdOfOldSent : annIdOfOldSentences) {
				if(annIdOfOldSent != null) {
					Annotation sentenceAnn = sentenceMarkup.get(inputSentenceAStypeAppo + "_OLD").get(annIdOfOldSent);
					if(sentenceAnn != null) {
						sentenceMarkup.remove(sentenceAnn);
					}
				}
			}
		}

	}
	
	@Override
	public boolean resetAnnotations() {
		// Delete PDFX as input type
		if(this.document.getFeatures() != null) {
			this.document.getFeatures().remove("source");
		}
		
		// Delete annotations imported from PDFX
		List<String> annTypesToRemove = new ArrayList<String>();
		annTypesToRemove.add(ImporterBase.abstractAnnType);
		annTypesToRemove.add(ImporterBase.bibEntryAnnType);
		annTypesToRemove.add(ImporterBase.h1AnnType);
		annTypesToRemove.add(ImporterBase.h2AnnType);
		annTypesToRemove.add(ImporterBase.h3AnnType);
		annTypesToRemove.add(ImporterBase.h4AnnType);
		annTypesToRemove.add(ImporterBase.h5AnnType);
		annTypesToRemove.add(ImporterBase.titleAnnType);
		annTypesToRemove.add(ImporterBase.captionAnnType);
		annTypesToRemove.add(ImporterBase.sentenceAnnType);
		
		for(String annTypeToRemove : annTypesToRemove) {
			List<Annotation> annListToRem = GateUtil.getAnnInDocOrder(this.document, ImporterBase.driAnnSet, annTypeToRemove);
			
			if(annListToRem != null && annListToRem.size() > 0) {
				for(Annotation annToRem : annListToRem) {
					if(annToRem != null) {
						this.document.getAnnotations(ImporterBase.driAnnSet).remove(annToRem);
					}
				}
			}
		}
		
		return true;
	}
}
